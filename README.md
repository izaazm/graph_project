# Extending InGram to Hyper-relational Facts
blablabla

##

### Reproducibility
We use a conda environenment as described in environment.yml. Furthermore we used a docker container build from the Dockerfile provided. To build the docker image run `docker build -t pytorch-gpu . -f Dockerfile`, and to run the container run the bash script `./run_container.sh`