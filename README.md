# Extending InGram to Hyper-relational Facts
blablabla

### Reproducibility
We use a conda environenment as described in environment.yml. Furthermore we used a docker container build from the Dockerfile provided on `pytorch 2.3.0` with `cuda 12.1`. To build the docker image run `docker build -t pytorch-gpu . -f Dockerfile`, and to run the container run the bash script `./run_container.sh`